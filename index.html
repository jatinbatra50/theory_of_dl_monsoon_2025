<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>STCS TIFR CSS.428.1, Introduction to Theory of Deep Learning, Monsoon 2025</title>

  <meta name="description" content="Course website for 'Introduction to Theory of Deep Learning' (CSS.428.1, Monsoon 2025). Overview, logistics, prerequisites, evaluation, references, and weekly outline." />
  <link rel="stylesheet" href="styles.css" />

  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Course",
    "name": "STCS TIFR CSS.428.1, Introduction to Theory of Deep Learning, Monsoon 2025",
    "courseCode": "CSS.428.1",
    "educationalLevel": "Graduate",
    "provider": {
      "@type": "CollegeOrUniversity",
      "name": "TIFR (School of Technology and Computer Science)"
    },
    "description": "A theoretical introduction to modern deep learning: generalization, optimization, overparameterization, and related topics.",
    "timeRequired": "P1.5H",
    "coursePrerequisites": "Comfort with linear algebra, probability, and basic optimization.",
    "keywords": "deep learning theory, generalization, optimization, implicit bias"
  }
  </script>
</head>
<body>
  <header class="site-header">
    <div class="container">
      <h1 class="title">STCS TIFR CSS.428.1, Introduction to Theory of Deep Learning, Monsoon 2025</h1>
      <p class="tagline">Course website</p>
      <nav class="quick-links">
        <a href="assets/Proposal_for_Introduction_to_Theory_of_Deep_Learning.pdf">Proposal (PDF)</a>
        <a href="#overview">Overview</a>
        <a href="#logistics">Logistics</a>
        <a href="#prereqs">Prerequisites</a>
        <a href="#evaluation">Evaluation</a>
        <a href="#outline">Topics</a>
        <a href="#reading">References</a>
        <a href="#contact">Contact</a>
      </nav>
    </div>
  </header>

  <main class="container">
    <section id="overview" class="card">
      <h2>Overview</h2>
      <p>
        This course develops a rigorous foundation for understanding modern deep learning.
        We will study generalization, optimization dynamics (including SGD), overparameterization
        and interpolation, implicit bias, and links to classical learning theory and statistics.
      </p>
    </section>

    <section id="logistics" class="card">
      <h2>Logistics</h2>
      <ul class="facts">
        <li><strong>Meeting time:</strong> Thursdays, 2:00–3:30 PM</li>
        <li><strong>Format:</strong> One lecture per week</li>
        <li><strong>Location:</strong> <em>tbd</em></li>
        <li><strong>Instructor:</strong> Jatin Batra (STCS, TIFR)</li>
        <li><strong>Teaching Assistant(s):</strong> <em>tbd</em></li>
        <li><strong>First lecture:</strong> <em>tbd</em></li>
        <li><strong>Office hours:</strong> <em>tbd</em></li>
      </ul>
    </section>

    <section id="prereqs" class="card">
      <h2>Prerequisites</h2>
      <p>
        Comfort with linear algebra, probability, and optimization. Some background in learning theory
        and basic deep learning (e.g., backprop) is helpful but not strictly required.
      </p>
    </section>

    <section id="evaluation" class="card">
      <h2>Evaluation</h2>
      <ul>
        <li>Problem sets: <em>tbd</em>%</li>
        <li>Project (report + presentation): <em>tbd</em>%</li>
        <li>Participation: <em>tbd</em>%</li>
        <li>Optional exam: <em>tbd</em>%</li>
      </ul>
      <p class="muted">
        Collaboration policy and late policy: <em>tbd</em>.
      </p>
    </section>

    <section id="outline" class="card">
      <h2>Topics &amp; Weekly Outline</h2>
      <ol class="weeks">
        <li><strong>Week 1:</strong> Foundations of statistical learning; generalization &amp; uniform convergence</li>
        <li><strong>Week 2:</strong> Optimization in overparameterized models; SGD dynamics; implicit bias</li>
        <li><strong>Week 3:</strong> Neural tangent kernel and linearization</li>
        <li><strong>Week 4:</strong> Overparameterization and interpolation; double descent</li>
        <li><strong>Week 5:</strong> Stability, compression, and generalization</li>
        <li><strong>Week 6:</strong> Depth vs. width; expressivity and approximation</li>
        <li><strong>Week 7:</strong> Optimization landscapes; critical points and saddle geometry</li>
        <li><strong>Week 8:</strong> Implicit regularization; margin maximization</li>
        <li><strong>Week 9:</strong> Generalization in large models; pretraining and transfer (theory)</li>
        <li><strong>Week 10:</strong> Transformers as sequence models (theory perspectives)</li>
        <li><strong>Week 11:</strong> PAC‑Bayes and modern bounds</li>
        <li><strong>Week 12:</strong> Student project presentations / synthesis</li>
      </ol>
      <p class="muted">
        The outline is indicative; topics and pacing may evolve.
      </p>
    </section>

    <section id="reading" class="card">
      <h2>References &amp; Reading</h2>
      <ul class="reading-list">
        <li><em>Understanding Machine Learning: From Theory to Algorithms</em> — Shalev-Shwartz &amp; Ben‑David</li>
        <li><em>Foundations of Machine Learning</em> — Mohri, Rostamizadeh &amp; Talwalkar</li>
        <li>Selected lecture notes and research papers (to be announced).</li>
      </ul>
    </section>

    <section id="contact" class="card">
      <h2>Contact</h2>
      <p>
        Email: <a href="mailto:your.email@tifr.res.in">your.email@tifr.res.in</a>
      </p>
    </section>
  </main>

  <footer class="site-footer">
    <div class="container">
      <small>Last updated: 28 Aug 2025</small>
    </div>
  </footer>
</body>
</html>
