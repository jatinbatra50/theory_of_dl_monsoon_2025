**Instructor:** Jatin Batra  
**Schedule:** Thursdays, 2:00–3:30 PM  
**Room:** A-238  
**Email:** [jatinbatra50@gmail.com](mailto:jatinbatra50@gmail.com)  
**Textbook:** *Learning theory from first principles* — Francis Bach (Chapter 9 and relevant parts of Chapters 3, 4, 5, 7, 8, 12, 14).

## Course Description

This mini-course serves as a take-off point for research in theory of deep learning,
with neural networks as the prototypical setup, more specifically, the focus will be on implicit bias of overparameterized models. Generalization power
   of modern deep learning cannot be easily explained as the number of pa-
   rameters is often of the same order as the number of training samples. In
   this part, we will initiate an understanding of implicit bias which captures
   additional properties enjoyed by natural training procedures for overpa-
   rameterized models that are not captured explicitly, and which could hold
   the key to generalization.

## Weekly Outline

| Week | Topic | Notes |
|:---:|:-----------------|:-----:|
| 01 | Implicit bias of Gradient Flow in Overparameterized Linear Regression | [Notes (PDF)](assets/notes/week-01.pdf) |
| 02 | A Glimpse of Double Descent | [Notes (PDF)](assets/notes/week-02.pdf) |
| 03 | Mickey Mouse Proof of Double Descent | [Notes (PDF)](assets/notes/week-03.pdf) |
| 04 | Topic | [Notes (PDF)](assets/notes/week-04.pdf) |
| 05 | Topic | [Notes (PDF)](assets/notes/week-05.pdf) |
| 06 | Topic | [Notes (PDF)](assets/notes/week-06.pdf) |
| 07 | Topic | [Notes (PDF)](assets/notes/week-07.pdf) |
| 08 | Topic | [Notes (PDF)](assets/notes/week-08.pdf) |
| 09 | Topic | [Notes (PDF)](assets/notes/week-09.pdf) |
| 10 | Topic | [Notes (PDF)](assets/notes/week-10.pdf) |
| 11 | Topic | [Notes (PDF)](assets/notes/week-11.pdf) |
| 12 | Topic | [Notes (PDF)](assets/notes/week-12.pdf) |
| 13 | Topic | [Notes (PDF)](assets/notes/week-13.pdf) |
| 14 | Topic | [Notes (PDF)](assets/notes/week-14.pdf) |
| 15 | Topic | [Notes (PDF)](assets/notes/week-15.pdf) |
